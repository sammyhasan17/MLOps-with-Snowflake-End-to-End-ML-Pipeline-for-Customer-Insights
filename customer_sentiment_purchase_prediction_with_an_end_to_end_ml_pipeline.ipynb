{
  "metadata": {
    "kernelspec": {
      "display_name": "Streamlit Notebook",
      "name": "streamlit"
    },
    "lastEditStatus": {
      "notebookId": "5zkdyeiyty73dlhr2cfn",
      "authorId": "854700334132",
      "authorName": "SAMMYHASAN17",
      "authorEmail": "sammyhasan17@gmail.com",
      "sessionId": "e70e10ad-3daf-4a02-8b7e-ba82fa444853",
      "lastEditTime": 1752890786323
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sammyhasan17/MLOPs_Snowflake/blob/main/customer_sentiment_purchase_prediction_with_an_end_to_end_ml_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91e3fdc8-8480-41c1-8d8a-6f7eb7101ad7",
      "metadata": {
        "collapsed": false,
        "name": "Setup",
        "id": "91e3fdc8-8480-41c1-8d8a-6f7eb7101ad7"
      },
      "source": [
        "### Initialize Environment for Distributed Processing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9c293b3-2fb8-4157-8ac9-fff85ac86a3b",
      "metadata": {
        "name": "cell26",
        "collapsed": false,
        "id": "f9c293b3-2fb8-4157-8ac9-fff85ac86a3b"
      },
      "source": [
        "# Installing the Snowflake Pyhon SKD"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mTCPFy6PGRsg"
      },
      "id": "mTCPFy6PGRsg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0c0dd10-192c-4f79-940c-ca675028b32c",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": true,
        "language": "python",
        "name": "cell19",
        "id": "c0c0dd10-192c-4f79-940c-ca675028b32c"
      },
      "outputs": [],
      "source": [
        "# Some of the features in this HOL are in preview. Pin the version of snowflake-ml-python for reproducibility.\n",
        "# NOTE - you do not need to restart the kernel since we're running this before importing snowflake-ml-python.\n",
        "!pip install snowflake-ml-python==1.9.0\n",
        "!pip install tf-keras\n",
        "\n",
        "# Note: Some MLops features are dependant on Using Snowflake Enterprise edition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7c17443-8a67-4250-9d12-6e08ab614811",
      "metadata": {
        "language": "python",
        "name": "cell8",
        "id": "a7c17443-8a67-4250-9d12-6e08ab614811"
      },
      "outputs": [],
      "source": [
        "!pip freeze | grep keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3300af7-7d13-43e6-b05b-0b20d3ffa5ce",
      "metadata": {
        "language": "python",
        "name": "cell39",
        "id": "e3300af7-7d13-43e6-b05b-0b20d3ffa5ce"
      },
      "outputs": [],
      "source": [
        "!pip freeze | grep snowflake # get snowflake sdk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91117e3b-e018-45ba-a547-0001558b0c5a",
      "metadata": {
        "language": "python",
        "name": "cell4",
        "id": "91117e3b-e018-45ba-a547-0001558b0c5a"
      },
      "outputs": [],
      "source": [
        "import ray\n",
        "import logging\n",
        "logging.getLogger().setLevel(logging.WARNING)\n",
        "\n",
        "\n",
        "context = ray.data.DataContext.get_current()\n",
        "context.execution_options.verbose_progress = False\n",
        "context.enable_operator_progress_bars = False\n",
        "context.enable_progress_bars = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3775908f-ca36-4846-8f38-5adca39217f2",
      "metadata": {
        "language": "python",
        "name": "cell1",
        "id": "3775908f-ca36-4846-8f38-5adca39217f2"
      },
      "outputs": [],
      "source": [
        "# Import python packages\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "\n",
        "# We can also use Snowpark for our analyses\n",
        "# # Create an active Snowflake session to run SQL queries and access Snowflake data in this notebook!\n",
        "from snowflake.snowpark.context import get_active_session\n",
        "session = get_active_session()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "601cf1ec-6b27-4604-9f81-119455b1d4d5",
      "metadata": {
        "language": "python",
        "name": "cell24",
        "id": "601cf1ec-6b27-4604-9f81-119455b1d4d5"
      },
      "outputs": [],
      "source": [
        "from snowflake.ml.runtime_cluster import scale_cluster\n",
        "\n",
        "# Scale out the notebook to have multiple nodes available for execution\n",
        "SCALE_FACTOR = 2\n",
        "# scale_cluster(SCALE_FACTOR) # NOTE: REQUIRES FULL VERSION\n",
        "\n",
        "# Sync the python env to the scaled out cluster.\n",
        "from runtime_env import python_env\n",
        "# python_env.sync_env() # refer to line 5"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9a0ea63-361a-49b8-a001-f70ecfa800cd",
      "metadata": {
        "collapsed": false,
        "name": "Process_Reviews",
        "id": "d9a0ea63-361a-49b8-a001-f70ecfa800cd"
      },
      "source": [
        "# Process Review Text Data\n",
        "- Load reviews with `SFStageTextDataSource`\n",
        "- Parse review text with Ray data"
      ]
    },
    {
      "cell_type": "code",
      "id": "fe7f483e-43bb-4607-bb8f-a83f9dfbfad6",
      "metadata": {
        "language": "sql",
        "name": "cell9",
        "id": "fe7f483e-43bb-4607-bb8f-a83f9dfbfad6"
      },
      "outputs": [],
      "source": [
        "SHOW STAGES IN SCHEMA HOL_DB.HOL_SCHEMA;\n"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "id": "442b6a20-eacf-432d-93fe-9c217c389c2a",
      "metadata": {
        "language": "sql",
        "name": "cell27",
        "id": "442b6a20-eacf-432d-93fe-9c217c389c2a"
      },
      "outputs": [],
      "source": [
        "SELECT CURRENT_ROLE();\n",
        "SHOW STAGES;\n",
        "/* check current role */"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c695373e-ac74-4b62-a1f1-08206cbd5c81",
      "metadata": {
        "codeCollapsed": false,
        "language": "python",
        "name": "cell3",
        "id": "c695373e-ac74-4b62-a1f1-08206cbd5c81"
      },
      "outputs": [],
      "source": [
        "# use ray data to process sentiment\n",
        "from snowflake.ml.ray.datasource import SFStageTextDataSource\n",
        "\n",
        "file_name = \"*.txt\"\n",
        "stage_name = \"REVIEWS\"\n",
        "\n",
        "\n",
        "text_source = SFStageTextDataSource(\n",
        "    stage_location=stage_name,\n",
        "    file_pattern=file_name\n",
        ")\n",
        "\n",
        "text_dataset = ray.data.read_datasource(text_source)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bdfae7c2-bc34-4a8d-a86f-e4a145e647e6",
      "metadata": {
        "language": "python",
        "name": "cell5",
        "id": "bdfae7c2-bc34-4a8d-a86f-e4a145e647e6"
      },
      "outputs": [],
      "source": [
        "def parse_reviews(batch):\n",
        "    \"\"\"\n",
        "    Parse reviews to extract UUID and review text from the input string.\n",
        "\n",
        "    Args:\n",
        "        batch: Dictionary containing 'text' and 'file_name' keys\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with parsed UUID and review text\n",
        "    \"\"\"\n",
        "    # Initialize empty dictionary for results\n",
        "    parsed_data = {}\n",
        "\n",
        "    value = batch[\"text\"]\n",
        "    # Split on the first occurrence of comma\n",
        "    parts = value.split('\",\"', 1)\n",
        "\n",
        "    # Clean up the UUID (remove leading/trailing quotes)\n",
        "    uuid = parts[0].strip('\"')\n",
        "\n",
        "    # Clean up the review text (remove trailing quote)\n",
        "    review_text = parts[1].rstrip('\"')\n",
        "\n",
        "    # Store parsed values\n",
        "    parsed_data['UUID'] = uuid\n",
        "    parsed_data['REVIEW_TEXT'] = review_text\n",
        "\n",
        "    return parsed_data\n",
        "\n",
        "# Apply the parsing function to the dataset\n",
        "parsed_dataset = text_dataset.map(parse_reviews)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41fae9e1-749d-4675-bb64-2ead05c798ab",
      "metadata": {
        "collapsed": false,
        "name": "Review_Quality",
        "id": "41fae9e1-749d-4675-bb64-2ead05c798ab"
      },
      "source": [
        "# Predict Review Quality\n",
        "- Predict the quality with one-shot classification via HF pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a9fdb90-1bae-4216-b910-b413f78e1b09",
      "metadata": {
        "language": "python",
        "name": "cell23",
        "id": "4a9fdb90-1bae-4216-b910-b413f78e1b09"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class ModelPredictor:\n",
        "    def __init__(self):\n",
        "        # Load model\n",
        "        self.classifier = pipeline(\"zero-shot-classification\",\n",
        "                      model=\"facebook/bart-large-mnli\")\n",
        "\n",
        "    # define your batch operations\n",
        "    def __call__(self, batch):\n",
        "        candidate_labels = ['detailed with specific information and experience', 'basic accurate information', 'generic brief with no details']\n",
        "        resp = self.classifier(batch[\"REVIEW_TEXT\"].tolist(), candidate_labels)\n",
        "\n",
        "        # Handle both resp and batch results\n",
        "        if isinstance(resp, dict):\n",
        "            raise ValueError(f\"Expected batch response, got {resp} for batch {batch['REVIEW_TEXT']}\")\n",
        "\n",
        "        # Add results to batch\n",
        "        batch[\"REVIEW_QUALITY\"] = np.array([result[\"labels\"][np.argmax(result[\"scores\"])] for result in resp])\n",
        "\n",
        "\n",
        "        return batch\n",
        "\n",
        "# Apply batch operations to your dataset. HF Pipeline is itself a batch operation, so we use Ray data just to scale across nodes, setting concurrency to number of nodes we have started.\n",
        "dataset = parsed_dataset.map_batches(ModelPredictor, concurrency=1, batch_size=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa4b3b9c-3430-45db-92dc-bb1b29eae790",
      "metadata": {
        "collapsed": false,
        "name": "Store_Reviews",
        "id": "fa4b3b9c-3430-45db-92dc-bb1b29eae790"
      },
      "source": [
        "# Store Processed Data in Snowflake"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2424713b-83ff-43b9-9f62-d3ba3ce82bdf",
      "metadata": {
        "language": "python",
        "name": "cell10",
        "id": "2424713b-83ff-43b9-9f62-d3ba3ce82bdf"
      },
      "outputs": [],
      "source": [
        "from snowflake.ml.ray.datasink.table_data_sink import SnowflakeTableDatasink\n",
        "\n",
        "datasink = SnowflakeTableDatasink(\n",
        "    table_name=\"REVIEWS\",\n",
        "    auto_create_table=True,\n",
        "    override=False,\n",
        "    )\n",
        "dataset.write_datasink(datasink)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6557fb5c-2bf2-4795-8d9c-c01bd7e09012",
      "metadata": {
        "language": "sql",
        "name": "cell11",
        "id": "6557fb5c-2bf2-4795-8d9c-c01bd7e09012"
      },
      "outputs": [],
      "source": [
        "show tables;"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "147ea8ea-1ee3-48f2-8a39-55beedc463ee",
      "metadata": {
        "collapsed": false,
        "name": "Sentiment_Analysis",
        "id": "147ea8ea-1ee3-48f2-8a39-55beedc463ee"
      },
      "source": [
        "# Execute Sentiment Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10972b8f-f098-40fe-b498-94f6f8233bf0",
      "metadata": {
        "language": "sql",
        "name": "cell2",
        "id": "10972b8f-f098-40fe-b498-94f6f8233bf0"
      },
      "outputs": [],
      "source": [
        "ALTER TABLE\n",
        "  REVIEWS\n",
        "ADD\n",
        "  COLUMN if not exists REVIEW_SENTIMENT FLOAT;\n",
        "\n",
        "UPDATE\n",
        "    REVIEWS\n",
        "SET REVIEW_SENTIMENT = (\n",
        "    SELECT CASE\n",
        "        WHEN sentiment_str = 'positive' THEN 1.0\n",
        "        WHEN sentiment_str = 'negative' THEN -1.0\n",
        "        WHEN sentiment_str = 'neutral' THEN 0.0\n",
        "        WHEN sentiment_str = 'mixed' THEN 0.5\n",
        "        ELSE 0.0  -- Default for any unexpected values\n",
        "    END\n",
        "FROM (\n",
        "    SELECT SNOWFLAKE.CORTEX.ENTITY_SENTIMENT(REVIEWS.REVIEW_TEXT):categories[0]:sentiment::STRING AS sentiment_str\n",
        ") AS sentiment_data);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0411bbba-4bc9-4d3d-a869-5833ed3e23f8",
      "metadata": {
        "language": "sql",
        "name": "cell25",
        "id": "0411bbba-4bc9-4d3d-a869-5833ed3e23f8"
      },
      "outputs": [],
      "source": [
        "select * from reviews limit 10;"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3732afef-283a-4fdc-a8fa-90842c17194c",
      "metadata": {
        "collapsed": false,
        "name": "Feature_Engineering",
        "id": "3732afef-283a-4fdc-a8fa-90842c17194c"
      },
      "source": [
        "# Prepare Data for Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52b76e74-8991-4458-b5c9-9a6fec345c80",
      "metadata": {
        "language": "python",
        "name": "cell6",
        "id": "52b76e74-8991-4458-b5c9-9a6fec345c80"
      },
      "outputs": [],
      "source": [
        "tabular_data = session.table(\"TABULAR_DATA\")\n",
        "review_data = session.table(\"REVIEWS\")\n",
        "\n",
        "train_dataframe = tabular_data.join(\n",
        "    review_data,\n",
        "    review_data['UUID'] == tabular_data['UUID'],\n",
        "    'inner'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "026e49b9-f72f-4741-9785-5929bf139d76",
      "metadata": {
        "language": "python",
        "name": "cell34",
        "id": "026e49b9-f72f-4741-9785-5929bf139d76"
      },
      "outputs": [],
      "source": [
        "train_dataframe.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98d96873-358c-4b71-8a36-c7386f72b27f",
      "metadata": {
        "language": "python",
        "name": "cell7",
        "id": "98d96873-358c-4b71-8a36-c7386f72b27f"
      },
      "outputs": [],
      "source": [
        "train_dataframe.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7c8e4a8-60d0-43bd-bc5c-45b1218e5dee",
      "metadata": {
        "language": "python",
        "name": "cell14",
        "id": "e7c8e4a8-60d0-43bd-bc5c-45b1218e5dee"
      },
      "outputs": [],
      "source": [
        "# Encode review sentiment and review quality\n",
        "from snowflake.ml.modeling.preprocessing import LabelEncoder\n",
        "\n",
        "# Select the columns to encode\n",
        "columns_to_encode = [\"REVIEW_QUALITY\", \"PRODUCT_LAYOUT\"]\n",
        "\n",
        "# Initialize LabelEncoder for each column\n",
        "encoders = [LabelEncoder(input_cols=[col], output_cols=[f\"{col}_OUT\"]) for col in columns_to_encode]\n",
        "for encoder in encoders:\n",
        "    train_dataframe = encoder.fit(train_dataframe).transform(train_dataframe)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8276baac-1886-4b14-a43d-15c039c49173",
      "metadata": {
        "collapsed": false,
        "name": "Distributed_Training",
        "id": "8276baac-1886-4b14-a43d-15c039c49173"
      },
      "source": [
        "# Train an XGBoost Model\n",
        "- Trains an XGBoost model over two nodes using Snowflake distributed `XGBEstimator`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b40f4747-43c7-4601-ba2a-55353b7743b8",
      "metadata": {
        "language": "python",
        "name": "cell20",
        "id": "b40f4747-43c7-4601-ba2a-55353b7743b8"
      },
      "outputs": [],
      "source": [
        "from snowflake.ml.modeling.distributors.xgboost import XGBEstimator, XGBScalingConfig\n",
        "from snowflake.ml.data.data_connector import DataConnector\n",
        "\n",
        "INPUT_COLS = [\"REVIEW_QUALITY_OUT\", \"PRODUCT_LAYOUT_OUT\", \"PAGE_LOAD_TIME\", \"REVIEW_SENTIMENT\", \"PRODUCT_RATING\"]\n",
        "LABEL_COL = 'PURCHASE_DECISION'\n",
        "\n",
        "params = {\n",
        "    \"eta\": 0.1,\n",
        "    \"max_depth\": 8,\n",
        "    \"min_child_weight\": 100,\n",
        "    \"tree_method\": \"hist\",\n",
        "}\n",
        "\n",
        "scaling_config = XGBScalingConfig(\n",
        "    use_gpu=False\n",
        ")\n",
        "\n",
        "estimator = XGBEstimator(\n",
        "    n_estimators=50,\n",
        "    objective=\"reg:squarederror\",\n",
        "    params=params,\n",
        "    scaling_config=scaling_config,\n",
        ")\n",
        "\n",
        "\n",
        "dc = DataConnector.from_dataframe(train_dataframe)\n",
        "xgb_model = estimator.fit(\n",
        "    dc, input_cols=INPUT_COLS, label_col=LABEL_COL\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "487da425-c275-4067-ba7d-ac358a4edb4a",
      "metadata": {
        "collapsed": false,
        "name": "Register_And_Deploy",
        "id": "487da425-c275-4067-ba7d-ac358a4edb4a"
      },
      "source": [
        "# Register and Deploy the Model\n",
        "- Register model to Snowflake Model Registry\n",
        "- Deploy code outside of notebook using ML Jobs"
      ]
    },
    {
      "cell_type": "code",
      "id": "8b3971c1-6ce0-4d31-82a6-dd7ff981d1e1",
      "metadata": {
        "language": "sql",
        "name": "cell30",
        "id": "8b3971c1-6ce0-4d31-82a6-dd7ff981d1e1"
      },
      "outputs": [],
      "source": [
        "SHOW DATABASES LIKE 'HOL_DB';\n",
        "SHOW SCHEMAS IN DATABASE HOL_DB;\n"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "id": "d9fa360c-a1cc-4b93-ad6b-481e2d7a3c21",
      "metadata": {
        "language": "sql",
        "name": "cell44",
        "id": "d9fa360c-a1cc-4b93-ad6b-481e2d7a3c21"
      },
      "outputs": [],
      "source": [
        "SELECT CURRENT_ROLE();"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1dc05873-face-4b13-adb5-239fa1d93437",
      "metadata": {
        "language": "python",
        "name": "cell21",
        "id": "1dc05873-face-4b13-adb5-239fa1d93437"
      },
      "outputs": [],
      "source": [
        "from snowflake.ml.registry import registry\n",
        "reg = registry.Registry(session=session)\n",
        "\n",
        "# Log the model in Snowflake Model Registry\n",
        "model_ref = reg.log_model(\n",
        "    model_name=\"deployed_xgb\",\n",
        "    model=xgb_model,\n",
        "    conda_dependencies=[\"scikit-learn\",\"xgboost\"],\n",
        "    sample_input_data=train_dataframe.select(INPUT_COLS),\n",
        "    comment=\"XGBoost model for forecasting customer demand\",\n",
        "    options= {\"enable_explainability\": True},\n",
        "    target_platforms = [\"WAREHOUSE\"]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39463ae9-e36a-4593-bc0d-4870a76837b8",
      "metadata": {
        "collapsed": false,
        "name": "Feature_Importance",
        "id": "39463ae9-e36a-4593-bc0d-4870a76837b8"
      },
      "source": [
        "# Assess Feature Importance with Explainability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc512677-adf6-4705-bbba-7baa273cdf37",
      "metadata": {
        "language": "python",
        "name": "cell12",
        "id": "bc512677-adf6-4705-bbba-7baa273cdf37"
      },
      "outputs": [],
      "source": [
        "explanations = model_ref.run(train_dataframe.select(INPUT_COLS), function_name=\"explain\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66b0eb01-c66c-4d42-958b-937b56d3f4c0",
      "metadata": {
        "language": "python",
        "name": "cell18",
        "id": "66b0eb01-c66c-4d42-958b-937b56d3f4c0"
      },
      "outputs": [],
      "source": [
        "explanations"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5442f277-f413-4a26-9e6d-1598985d4079",
      "metadata": {
        "collapsed": false,
        "name": "Deploy_Jobs",
        "id": "5442f277-f413-4a26-9e6d-1598985d4079"
      },
      "source": [
        "# Deploy To Production"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a71509e0-021f-49e0-9d9f-02049c37a2eb",
      "metadata": {
        "language": "python",
        "name": "cell29",
        "id": "a71509e0-021f-49e0-9d9f-02049c37a2eb"
      },
      "outputs": [],
      "source": [
        "from snowflake.ml.jobs import remote\n",
        "@remote(compute_pool=\"SYSTEM_COMPUTE_POOL_CPU\", stage_name=\"payload_stage\", external_access_integrations=[\"ALLOW_ALL_ACCESS_INTEGRATION\"])\n",
        "def update_reviews():\n",
        "    import ray\n",
        "    from snowflake.ml.ray.datasink.table_data_sink import SnowflakeTableDatasink\n",
        "    from snowflake.ml.ray.datasource import SFStageTextDataSource\n",
        "\n",
        "    file_name = \"*.txt\"\n",
        "    stage_name = \"REVIEWS\"\n",
        "\n",
        "    text_source = SFStageTextDataSource(\n",
        "        stage_location=stage_name,\n",
        "        file_pattern=file_name\n",
        "    )\n",
        "\n",
        "    text_dataset = ray.data.read_datasource(text_source)\n",
        "\n",
        "    # text_dataset = ray.data.read_datasource(text_source)\n",
        "    parsed_dataset = text_dataset.map(parse_reviews)\n",
        "    dataset = parsed_dataset.map_batches(ModelPredictor, concurrency=1, batch_size=10, num_cpus=24)\n",
        "\n",
        "    datasink = SnowflakeTableDatasink(\n",
        "        table_name=\"REVIEWS\",\n",
        "        auto_create_table=True,\n",
        "        override=False,\n",
        "        )\n",
        "    dataset.write_datasink(datasink)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35d2400a-ae2d-4371-a5d8-f20e1df96729",
      "metadata": {
        "language": "python",
        "name": "cell38",
        "id": "35d2400a-ae2d-4371-a5d8-f20e1df96729"
      },
      "outputs": [],
      "source": [
        "# Create a training job\n",
        "@remote(compute_pool=\"SYSTEM_COMPUTE_POOL_CPU\", stage_name=\"payload_stage\", external_access_integrations=[\"ALLOW_ALL_ACCESS_INTEGRATION\"])\n",
        "def retrain(session):\n",
        "    import datetime\n",
        "    from snowflake.ml.modeling.distributors.xgboost import XGBEstimator, XGBScalingConfig\n",
        "    from snowflake.ml.data.data_connector import DataConnector\n",
        "\n",
        "    tabular_data = session.table(\"HOL_DB.HOL_SCHEMA.TABULAR_DATA\")\n",
        "    review_data = session.table(\"HOL_DB.HOL_SCHEMA.REVIEWS\")\n",
        "\n",
        "    INPUT_COLS = [\"REVIEW_QUALITY_OUT\", \"PRODUCT_LAYOUT_OUT\", \"PAGE_LOAD_TIME\", \"REVIEW_SENTIMENT\", \"PRODUCT_RATING\"]\n",
        "    LABEL_COL = 'PURCHASE_DECISION'\n",
        "\n",
        "    train_dataframe = tabular_data.join(\n",
        "        review_data,\n",
        "        review_data['UUID'] == tabular_data['UUID'],\n",
        "        'inner'\n",
        "    )\n",
        "\n",
        "    # Encode review sentiment and review quality\n",
        "    from snowflake.ml.modeling.preprocessing import LabelEncoder\n",
        "\n",
        "    # Select the columns to encode\n",
        "    columns_to_encode = [\"REVIEW_QUALITY\", \"PRODUCT_LAYOUT\"]\n",
        "\n",
        "    # Initialize LabelEncoder for each column\n",
        "    encoders = [LabelEncoder(input_cols=[col], output_cols=[f\"{col}_OUT\"]) for col in columns_to_encode]\n",
        "    for encoder in encoders:\n",
        "        train_dataframe = encoder.fit(train_dataframe).transform(train_dataframe)\n",
        "\n",
        "    params = {\n",
        "        \"eta\": 0.1,\n",
        "        \"max_depth\": 8,\n",
        "        \"min_child_weight\": 100,\n",
        "        \"tree_method\": \"hist\",\n",
        "    }\n",
        "\n",
        "    scaling_config = XGBScalingConfig(\n",
        "        use_gpu=False\n",
        "    )\n",
        "\n",
        "    estimator = XGBEstimator(\n",
        "        n_estimators=50,\n",
        "        objective=\"reg:squarederror\",\n",
        "        params=params,\n",
        "        scaling_config=scaling_config,\n",
        "    )\n",
        "\n",
        "\n",
        "    dc = DataConnector.from_dataframe(train_dataframe)\n",
        "    xgb_model = estimator.fit(\n",
        "        dc, input_cols=INPUT_COLS, label_col=LABEL_COL\n",
        "    )\n",
        "\n",
        "    dc = DataConnector.from_dataframe(train_dataframe)\n",
        "    xgb_model = estimator.fit(\n",
        "        dc, input_cols=INPUT_COLS, label_col=LABEL_COL\n",
        "    )\n",
        "\n",
        "    from snowflake.ml.registry import registry\n",
        "    reg = registry.Registry(session=session)\n",
        "\n",
        "    # Log the model in Snowflake Model Registry\n",
        "    _ = reg.log_model(\n",
        "        model_name=\"CONVERSTION_CLASSIFIER\",\n",
        "        model=xgb_model,\n",
        "        version_name=f\"retrain_{datetime.datetime.now().strftime('v%Y%m%d_%H%M%S')}\",\n",
        "        conda_dependencies=[\"scikit-learn\",\"xgboost\"],\n",
        "        sample_input_data=train_dataframe.select(INPUT_COLS),\n",
        "        comment=\"XGBoost model for forecasting customer demand\",\n",
        "        options= {\"enable_explainability\": True},\n",
        "        target_platforms = [\"WAREHOUSE\"]\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4effe789-c052-4dd4-b28f-8614915f1bcf",
      "metadata": {
        "language": "python",
        "name": "cell16",
        "id": "4effe789-c052-4dd4-b28f-8614915f1bcf"
      },
      "outputs": [],
      "source": [
        "# You can run the job manually, and get the status and logs of the job\n",
        "train_job = retrain(session)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4748787c-ccdd-47c6-a5eb-5bebe1c5006a",
      "metadata": {
        "codeCollapsed": false,
        "language": "python",
        "name": "cell31",
        "id": "4748787c-ccdd-47c6-a5eb-5bebe1c5006a"
      },
      "outputs": [],
      "source": [
        "while train_job.status == \"PENDING\":\n",
        "    time.sleep(1)\n",
        "\n",
        "# Once job starts running, we can view the logs\n",
        "train_job.get_logs()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb5731e7-6495-4801-bb66-df0f6689ca68",
      "metadata": {
        "language": "python",
        "name": "cell17",
        "id": "fb5731e7-6495-4801-bb66-df0f6689ca68"
      },
      "outputs": [],
      "source": [
        "# we can also see all the jobs, and manage them with the job manager\n",
        "from snowflake.ml import jobs\n",
        "\n",
        "all_jobs = jobs.list_jobs()\n",
        "\n",
        "mask = all_jobs['status'].str.contains(\"FAILED\")\n",
        "filtered_df = all_jobs[mask]\n",
        "\n",
        "job_names = filtered_df[\"name\"]\n",
        "for id in job_names:\n",
        "    jobs.delete_job(id)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "483b0c5f-e594-4dfc-be95-ebfd2076f71d",
      "metadata": {
        "collapsed": false,
        "name": "Automate_Pipeline",
        "id": "483b0c5f-e594-4dfc-be95-ebfd2076f71d"
      },
      "source": [
        "# Create Automated ML Pipeline\n",
        "- Automate the deployment of the pipeline using Snowflake Tasks\n",
        "- After DAG creation, navigate to Monitoring -> Task History to view execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4a3b669-d13e-41f6-8106-93eda2de7ad1",
      "metadata": {
        "language": "python",
        "name": "cell15",
        "id": "d4a3b669-d13e-41f6-8106-93eda2de7ad1"
      },
      "outputs": [],
      "source": [
        "from snowflake.core.task.dagv1 import DAG, DAGTask\n",
        "from snowflake.core.task.context import TaskContext\n",
        "from datetime import timedelta\n",
        "from snowflake.snowpark import Session\n",
        "import snowflake.ml.jobs.manager as manager\n",
        "import datetime\n",
        "import json\n",
        "\n",
        "WAREHOUSE = session.get_current_warehouse()\n",
        "\n",
        "\n",
        "def refresh_reviews(session: Session) -> None:\n",
        "    job = update_reviews()\n",
        "    # Throw error if job fails\n",
        "    final_status = job.wait()\n",
        "\n",
        "    if final_status == \"FAILED\":\n",
        "        raise RuntimeError(f\"Job {job} failed with logs \")\n",
        "\n",
        "def update_sentiment(session: Session) -> None:\n",
        "    sql_text = \"\"\"\n",
        "        UPDATE REVIEWS\n",
        "        SET REVIEW_SENTIMENT = (\n",
        "        SELECT CASE\n",
        "            WHEN sentiment_str = 'positive' THEN 1.0\n",
        "            WHEN sentiment_str = 'negative' THEN -1.0\n",
        "            WHEN sentiment_str = 'neutral' THEN 0.0\n",
        "            WHEN sentiment_str = 'mixed' THEN 0.5\n",
        "            ELSE 0.0  -- Default for any unexpected values\n",
        "        END\n",
        "        FROM (\n",
        "            SELECT SNOWFLAKE.CORTEX.ENTITY_SENTIMENT(REVIEWS.REVIEW_TEXT):categories[0]:sentiment::STRING AS sentiment_str\n",
        "        ) AS sentiment_data);\n",
        "    \"\"\"\n",
        "    session.sql(sql_text).collect()\n",
        "\n",
        "def retrain_model(session: Session) -> None:\n",
        "    job = retrain(session)\n",
        "    # Throw error if job fails\n",
        "    final_status = job.wait()\n",
        "\n",
        "    if final_status == \"FAILED\":\n",
        "        raise RuntimeError(f\"Job {job} failed with logs \")\n",
        "\n",
        "def setup(session: Session) -> str:\n",
        "    info = dict(\n",
        "        run_id=datetime.datetime.now().strftime(\"v%Y%m%d_%H%M%S\"),\n",
        "    )\n",
        "    return json.dumps(info)\n",
        "\n",
        "def create_dag() -> DAG:\n",
        "    with DAG(\n",
        "        \"review_model_dag\",\n",
        "        warehouse=WAREHOUSE,\n",
        "        schedule=timedelta(days=1),\n",
        "        stage_location=\"payload_stage\",\n",
        "        packages=[\"snowflake-snowpark-python\", \"snowflake-ml-python==1.8.6\", \"transformers\"]\n",
        "    ) as dag:\n",
        "        # Need to wrap first function in a DAGTask to make >> operator work properly\n",
        "        setup_task = DAGTask(\"setup\", definition=setup)\n",
        "\n",
        "        # Build the DAG\n",
        "        setup_task >> refresh_reviews >> update_sentiment >> retrain_model\n",
        "\n",
        "    return dag\n",
        "\n",
        "from snowflake.core import CreateMode, Root\n",
        "from snowflake.core.task.dagv1 import DAGOperation\n",
        "api_root = Root(session)\n",
        "\n",
        "dag_op = DAGOperation(\n",
        "    schema=api_root.databases[session.get_current_database()].schemas[session.get_current_schema()]\n",
        ")\n",
        "\n",
        "dag = create_dag()\n",
        "dag_op.deploy(dag, mode=CreateMode.or_replace)\n",
        "dag_op.run(dag)\n",
        "\n",
        "current_runs = dag_op.get_current_dag_runs(dag)\n",
        "for r in current_runs:\n",
        "    print(f\"RunId={r.run_id} State={r.state}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "530a9b3c-f4d1-4181-ba03-de8365a3f07e",
      "metadata": {
        "language": "sql",
        "name": "cell13",
        "id": "530a9b3c-f4d1-4181-ba03-de8365a3f07e"
      },
      "outputs": [],
      "source": [
        "show models;"
      ]
    }
  ]
}